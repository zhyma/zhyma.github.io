<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhaoyuan Ma</title>
  <icon>https://www.gravatar.com/avatar/46a3cc1a027d9c8b6495d3c63f0a302c</icon>
  <subtitle>Personal portfolio</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhyma.github.io/"/>
  <updated>2020-07-22T18:41:12.228Z</updated>
  <id>http://zhyma.github.io/</id>
  
  <author>
    <name>Zhaoyuan Ma</name>
    <email>zhaoyuanma23@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Chatting robot with behavior learning</title>
    <link href="http://zhyma.github.io/Chatting-robot-with-behavior-learning/"/>
    <id>http://zhyma.github.io/Chatting-robot-with-behavior-learning/</id>
    <published>2017-07-21T03:42:07.000Z</published>
    <updated>2020-07-22T18:41:12.228Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Katsushi Ikeuchi, David Baumert, Yutaka Suzue, Masaru Takizawa, Kazuhiro Sasabuchi, et al. </p><p><strong>My contribution:</strong></p><ul><li>Software design on top of ROS.</li></ul><p><strong>Related work was demoed at “Microsoft Research Faculty Summit 2017: The Edge of AI” (<a href="https://www.microsoft.com/en-us/research/event/microsoft-research-asia-academic-day-2017/" target="_blank" rel="noopener">Automatic Description of Human Motion and Its Reproduction by Robot Based on Labanotation</a>) and “2017 MSRA academic day”(<a href="https://www.microsoft.com/en-us/research/video/video-abstract-human-robot-collaboration/" target="_blank" rel="noopener">Human-Robot Collaboration</a>).</strong></p><p>We propose a cloud-based system to empower service robots to generate human-like motions corresponding to the on-going conversation while chatting with people. In conversation, a gesture along with spoken sentence is an important factor as it is referred to as body language. This is particularly true for humanoid service robots because the key merit of such a humanoid robot is its resemblance to human shape as well as human behavior. Currently, robots with physical forms are not expected to reply with gestures when encountering contents outside their fixed knowledge. To fill this gap, we design a working prototype by employing a SoftBank Pepper robot and Microsoft Cognitive Services, shown in the diagram below.</p><p><img src="/img/msrabot/diag1.png" alt=""></p><p>To enable the gesture synthesis function, we build a “gesture library”. First, it contains common gestures. Second, it can analyze a sentence to map it a concept within the library. Third, it searches in the library for the concept, select a suitable gesture, which is written in Labanotation, to drive robots. The system diagram is shown below.</p><p><img src="/img/msrabot/diag2.png" alt=""></p><p>To prepare the library, we collected common gestures of daily conversations by using our Learning-from-Observation system (you can find more details <a href="/Robot-learning-from-observation/" title="here">here</a>), clustered them into about 40 concepts and designed a mapping mechanism to pair given sentences to concepts. To implement a complete system, we also employed Microsoft face tracking, speech recognition, chatting engine and text to speech services. Thus, our robots are able to process conversations with humans and automatically generate meaningful physical behaviors to accompany its spoken words.</p><p>You can find more details about this project from <a href="https://www.microsoft.com/en-us/research/video/visual-sensing-visual-intelligence/" target="_blank" rel="noopener">Katsu’s talk</a> at Microsoft Research Faculty Summit 2017 (starts at 48:26). Related slides can be found <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/07/From_Visual_Sensing_to_Visual_Intelligence_Katsu_Ikeuchi.pdf" target="_blank" rel="noopener">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/msrabot/pepper.jpg&quot;&gt; A robot system that is able to automatically generate life-like and meaningful physical behaviors to accompany its spoken words when process conversations with humans.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Robotics" scheme="http://zhyma.github.io/tags/Robotics/"/>
    
  </entry>
  
  <entry>
    <title>Robot learning from observation</title>
    <link href="http://zhyma.github.io/Robot-learning-from-observation/"/>
    <id>http://zhyma.github.io/Robot-learning-from-observation/</id>
    <published>2016-05-26T14:25:52.000Z</published>
    <updated>2020-07-22T18:41:31.160Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Teamwork with</strong> Katsushi Ikeuchi, Zengquang Yan, Yoshihiro Sato, Minako Nakamura, Shunsuke Kudho, et al.</p><p><strong>My contribution:</strong></p><ul><li>Mechanical design and prototyping</li><li>Robot control circuit design and prototyping</li><li>Robot control programming under ROS</li><li>improving algorithm which generates Labanotation from Kinect’s skeleton information</li></ul><p><strong>Related work is published in International Journal of Computer Vision, 2018. You can find out more information <a href="http://link.springer.com/article/10.1007/s11263-018-1123-1" target="_blank" rel="noopener">here</a>.</strong><br><strong>Related work was demoed to Bill Gates, Paul Allen and Satya Nadella at TechFest 2016 of Microsoft.</strong></p><div style="display:inline-block;text-align:center"><img src="/img/msrabot/robot@facebook2.png" alt=""><br><em>Related post on Harry Shum’s facebook</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/msrabot/robot3.jpg" alt=""><br><em>Hand-made robot prototype</em><br></div><p>We present a robot interaction system by introducing <a href="https://en.wikipedia.org/wiki/Labanotation" target="_blank" rel="noopener">Labanotation</a> into the working process. In real world scenario, there are several ways of programming a robot. But due to the kinematic and dynamic difference between human and robots, or even different robots themselves, simple mimicking method to repeat exactly the same joint angles will not work. So we bring Labanotation, a notation being used to record dance, as an intermediate symbolic representation for imitation of human’s upper body motion. In our system, a person performs series of actions in front of a Kinect first. Human skeleton information is captured and processed into so-called “energy function”. Peaks in the energy function are detected and treated as time point of key poses. A sequence of Labanotation is generated from the skeleton information (a.k.a. key pose) of the corresponding time points. After that, the Labanotation is sent to different robots. Robots receive the message then interpret Labanotation according to its’ own kinematics structure to mimic human action.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/msrabot/flowchart.png" alt=""><br><em>Procedures for using Labanotation as intermediate language of robot control.</em><br></div><p>For tasks that do not request precise manipulation, the biggest benefits of doing this is time-saving for motion editing to different robots. Engineers only need to program the robot once to set up rules of mapping Labanotation symbols. Then robots are able to interpret any Labanotation sequences.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/msrabot/robot.jpg&quot;&gt;Robots learn human motion by observing human behavior.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Robotics" scheme="http://zhyma.github.io/tags/Robotics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
  <entry>
    <title>Touch typing on touch cover</title>
    <link href="http://zhyma.github.io/Touch-typing-on-touch-cover/"/>
    <id>http://zhyma.github.io/Touch-typing-on-touch-cover/</id>
    <published>2015-07-07T15:03:41.000Z</published>
    <updated>2020-07-22T18:41:37.173Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Darren Edge, Leah Findlater, Hong Z. Tan</p><p><strong>My contribution:</strong></p><ul><li>Mechanical structure design and processing.</li><li>Circuit design, assembling and debugging.</li><li>Firmware and software programming and tuning.</li><li>Design and conducting a user study based on the keyboard.</li></ul><p><strong>Related work on this keyboard is published in Proceedings of IEEE World Haptics Conference (WHC 2015), you can find out more information <a href="https://www.microsoft.com/en-us/research/publication/haptic-keyclick-feedback-improves-typing-speed-and-reduces-typing-errors-on-a-flat-keyboard/" target="_blank" rel="noopener">here</a></strong></p><div style="display:inline-block;text-align:center;"><br><object width="560" height="315" align="aligncenter" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true"><param name="allowscriptaccess" value="always"><param name="src" value="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=100"><param name="allowfullscreen" value="true"><embed width="560" height="315" type="application/x-shockwave-flash" src="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=100" allowfullscreen="true" allowscriptaccess="always"></object><br><em>Demostrated on Microsoft TechFest 2014 (Youtube video)</em><br></div><p>People prefer Microsoft Type Cover rather than Touch Cover regardless of the thickness difference, due to Type Cover provide a better feeling of haptic feedback. Although this Cover shares the exact same appearance and dimension with other Touch Covers, everything inside has been pulled out and replaced by a totally different structure. Piezoelectric actuators are used to simulate physical keyboards, which means to create a sense of click under the certain finger that is pressing down. The study shows that people type faster and make fewer errors when they type on this kind of key-click as compared to just sound feedback.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/keyboard1.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/IMG_4783.jpg" alt=""><br><em>Our implementation: black. Original Touch Cover: blue</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/IMG_4794.jpg" alt=""><br><em>Sideview</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/insight.jpg" alt=""><br><em>The inside view of the keyboard structure (modified version for user study)</em><br></div><p><strong>Keyboard Feature:</strong></p><ul><li>Same form factor as Microsoft Touch Cover.</li><li>Isolated haptic feedback under each key.</li><li>Using piezo ceramic for both sensing and actuating.</li><li>Haptic feedback delay: 40.2ms.</li><li>Standard USB keyboard interface (through MicroUSB connection).</li><li>With configuration tool to turn on/off haptic feedback or change the feedback waveform.</li></ul><p><strong>User Study Take Away:</strong></p><ul><li>Haptic feedback improves typing speed and reduces errors</li><li>Audio feedback helps, but only if there is no haptics</li><li>Haptic feedback alone is superior to audio feedback alone</li><li>Users prefer localized haptics, but global helps home positioning</li></ul><p>Special thanks to Magnetro Chen for his help!</p><p>I proceeded a user study with 24 participants, more than 60 hours in total.<br><img src="/img/keyboard/27A3152-1.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/keyboard/keyboard.jpg&quot;&gt;This is the world first flat keyboard (without moving keys) that can deliver haptic keyclick feedback felt locally on each key. This keyboard has the same form factor of a Microsoft Touch Cover. We conducted a study to evaluate how haptic keyclick feedback might improve  typing  performance. &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/haptic-keyclick-feedback-improves-typing-speed-and-reduces-typing-errors-on-a-flat-keyboard/&quot;&gt;Related work&lt;/a&gt; on this keyboard is published on Proceedings of IEEE World Haptics Conference (WHC 2015).
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
  <entry>
    <title>eClover</title>
    <link href="http://zhyma.github.io/eClover/"/>
    <id>http://zhyma.github.io/eClover/</id>
    <published>2015-07-07T14:37:45.000Z</published>
    <updated>2020-07-22T18:41:19.733Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Yukang Yan, Darren Edge, Hong Z. Tan, Yuanchun Shi and Ed Colgate</p><p><strong>My contribution:</strong></p><ul><li>Circuit (tactors controlling) design, assembling and debugging.</li><li>Software modification.</li></ul><p><img src="/img/eclover/eclover-sys1.jpg" alt=""></p><p>We present a wearable prototype combining an electrostatic interface with four tactors worn around the wrist. This combination allows users to feel the approximate time in situations where glancing might not be appropriate, such as when in a meeting or giving a presentation. Users can also elect to receive notifications for scheduled events or at regular time intervals throughout the day.</p><p><strong>Driving circuit</strong><br>The driver for the electrostatic display and tactors.</p><p><strong>Electrostatic display</strong><br>The electrostatic display on the upper surface allows users to “pull” information at any time. The system provides relative information such as time elapsed by adjusting the extent of high and low friction areas along the length of the display.</p><p><strong>Tactors around the wrist</strong><br>Four tactors are arranged around the wrist, each controlled individually. We use the relative position of the signal generated by the tactors and electrostatic display to notify the user of certain timing milestones. We also use the cutaneous saltatory effect to generate vibration patterns that could be used to announce the time, e.g., every half hour.</p><p>This project is presented as a Demo in the <a href="http://haptics2015.org/program/index.html#Demonstrations" target="_blank" rel="noopener">IEEE World Haptics Conference 2015</a>.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/eclover/eclover-sys2.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><p><strong>Early prototypes:</strong></p><div style="display:inline-block;text-align:center;"><br><img src="/img/eclover/827A1663.jpg" alt=""><br><em>Up: tactors and their control unit.<br>Down: enclosed with leather (hand made)</em><br></div>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/eclover/eclover.jpg&quot;&gt;A wearable prototype combining an electrostatic interface with four tactors worn around the wrist. This combination allows users to feel the approximate time in situations where glancing might not be appropriate, such as when in a meeting or giving a presentation. Users can also elect to receive notifications for scheduled events or at regular time intervals throughout the day.
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
  <entry>
    <title>Haptics feedback on a touch screen</title>
    <link href="http://zhyma.github.io/Haptics-feedback-on-a-touch-screen/"/>
    <id>http://zhyma.github.io/Haptics-feedback-on-a-touch-screen/</id>
    <published>2014-05-04T03:33:47.000Z</published>
    <updated>2020-07-22T18:41:24.853Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Chen Zhao, Xiaowei Dai, Kwangtaek Kim, Jiawei Gu, Peter Choi, Hong Z. Tan, Ed Colgate</p><p><strong>My contribution:</strong></p><ul><li>Physical simulation</li><li>Circuit debugging</li><li>Piezoelectric actuators impedance matching</li><li>Firmware/APP programming</li></ul><div style="display:inline-block;text-align:center;"><br><object width="560" height="315" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true"><param name="allowscriptaccess" value="always"><param name="src" value="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=137"><param name="allowfullscreen" value="true"><embed width="560" height="315" type="application/x-shockwave-flash" src="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=137" allowfullscreen="true" allowscriptaccess="always"></object><br><em>Demostrated on Microsoft TechFest 2014 (Youtube video)</em><br></div><p>SlickFeel-Lumia is an advanced version of SlickFeel. A Lumia 920, a piece of glass with piezo actuators, circuit boards and a battery are boxed together to implement a handheld haptic feedback device. This device could generate different friction based on TPaD (tactile pattern display) technic, which allows users to create a different feeling on bare fingers for scenarios. For examples, switching between selections, scrolling on bars, moving out of range. The localized feeling it provides distinguished this device from phones using vibrators as haptic feedback.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/lumia/projLumia.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><p><img src="/img/lumia/DSC07467.jpg" alt=""></p><p><img src="/img/lumia/DSC07552.jpg" alt=""></p><p>Special thanks to Prof. Ed Colgate and the <a href="http://tpadtablet.org/" target="_blank" rel="noopener">TPaD</a> Tablet team for sharing their knowledge on TPaD with us!<br>This project is accepted as a Demo in the <a href="http://www.haptics2013.org/sub03_07.php" target="_blank" rel="noopener">IEEE World Haptics Conference 2013</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/lumia/lumia.jpg&quot;&gt;A handheld haptic feedback device which could generate different friction based on TPaD (tactile pattern display) technic, allow users to create a different feeling on bare fingers for scenarios. For examples, switching between selections, scrolling on bars, moving out of range. The localized feeling it provides distinguished this device from phones using vibrators as haptic feedback.
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
</feed>
