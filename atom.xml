<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhaoyuan Ma</title>
  <icon>https://www.gravatar.com/avatar/46a3cc1a027d9c8b6495d3c63f0a302c</icon>
  <subtitle>Personal portfolio</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhyma.github.io/"/>
  <updated>2023-12-25T21:55:55.665Z</updated>
  <id>http://zhyma.github.io/</id>
  
  <author>
    <name>Zhaoyuan Ma</name>
    <email>zhaoyuanma23@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Robotic Perception-Motion Synergy for Novel Rope Wrapping Tasks</title>
    <link href="http://zhyma.github.io/Robotic%20Perception-Motion%20Synergy%20for%20Novel%20Rope%20Wrapping%20Tasks/"/>
    <id>http://zhyma.github.io/Robotic Perception-Motion Synergy for Novel Rope Wrapping Tasks/</id>
    <published>2023-06-05T18:06:42.000Z</published>
    <updated>2023-12-25T21:55:55.665Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Jing Xiao</p><p><strong>This work is published on IEEE Robotics and Automation Letters and presented on IEEE-RAS HUMANOIDS 2023. You can find out the paper <a href="https://www.sae.org/publications/technical-papers/content/2022-01-0089/?_ga=2.186151533.537625167.1648003899-845020778.1643009504" target="_blank" rel="noopener">here</a>.</strong></p><p><strong>The digest video: (<a href="https://www.youtube.com/watch?v=BcXeCnQEwDQ" target="_blank" rel="noopener">YouTube link</a>)</strong></p><iframe width="560" height="315" src="https://www.youtube.com/embed/BcXeCnQEwDQ?si=yjw9KhAN-RYznRqN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><p>We introduce a novel and general method to address the problem of using a general-purpose robot manipulator with a parallel gripper to wrap a deformable linear object (DLO), called a rope, around a rigid object, called a rod, autonomously. Such a robotic wrapping task has broad potential applications in automotive, electromechanical industries, construction manufacturing, etc., but has hardly been studied. Our method does not require prior knowledge of the physical and geometrical properties of the objects but enables the robot to use real-time RGB-D perception to determine the wrapping state and feedback control to achieve high-quality results. As such, it provides the robot manipulator with the general capabilities to handle wrapping tasks of different rods or ropes. We tested our method on 6 combinations of 3 different ropes and 2 rods. The result shows that the wrapping quality improved and converged within 5 wraps for all test cases. </p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/rope1/rope1.jpg&quot;&gt;Introduce a method that uses a general-purpose robot manipulator to wrap a rope around a rod tightly purely based on perception. In the wrapping process, perception and motion are interleaving, no prior information about the rod and the rope is needed.&lt;br /&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/10137883&quot;&gt;&lt;b&gt;Related work&lt;/b&gt;&lt;/a&gt; is published on IEEE Robotics and Automation Letters.&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Parallel computing" scheme="http://zhyma.github.io/tags/Parallel-computing/"/>
    
      <category term="Optimal control" scheme="http://zhyma.github.io/tags/Optimal-control/"/>
    
  </entry>
  
  <entry>
    <title>Using GPU to Accelerate Vehicle Speed Optimal Control</title>
    <link href="http://zhyma.github.io/GPU-optimal-control/"/>
    <id>http://zhyma.github.io/GPU-optimal-control/</id>
    <published>2022-03-30T18:06:42.000Z</published>
    <updated>2023-12-25T21:53:52.993Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Xiangrui Zeng</p><p><strong>This work is published in SAE WCX 2022 as a techincal paper. You can find out more information <a href="https://www.sae.org/publications/technical-papers/content/2022-01-0089/?_ga=2.186151533.537625167.1648003899-845020778.1643009504" target="_blank" rel="noopener">here</a></strong></p><p>We proposes a method to adapt backward induction, which is used to solve the vehicle speed optimal control problem for energy efficiency, to a computer with a GPU to accelerate the computation. A common application of this type of problem is to control a vehicle on a given route with surrounding vehicles, road grades, traffic signals, stop signs, speed limits, and other conditions. Several indicators can be used to determine the performance of the controller, including the energy consumption of the trip, the driving speed smoothness, and the traveling time to a given destination. Solving this optimization problem globally by backward induction is time-consuming, due to the large searching space of the vehicle’s distance, velocity, and acceleration. The proposed method converts the single thread implementation to a parallel process that runs on a consumer-level GPU. This is done by choosing the problem scale, separating independent sub-processes, and pruning the data to accommodate the GPU programming requirement. The method is tested on a simulated route with a leading vehicle, a traffic light, and speed limits. The historical behaviors of the leading vehicle are known, and they are used to predict its future behaviors in a stochastic way. Compared to the CPU-based backward induction, the proposed GPU-based version solves the given problem 15 to 30 times faster, depending on the preset granularities of variables.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/gpu/road.jpg&quot;&gt;Using GPU to accelerate backward induction for vehicle speed optimal control.&lt;br /&gt;&lt;a href=&quot;https://www.sae.org/publications/technical-papers/content/2022-01-0089/&quot;&gt;&lt;b&gt;Related work&lt;/b&gt;&lt;/a&gt; is published on WCX SAE World Congress Experience 2022.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Parallel computing" scheme="http://zhyma.github.io/tags/Parallel-computing/"/>
    
      <category term="Optimal control" scheme="http://zhyma.github.io/tags/Optimal-control/"/>
    
  </entry>
  
  <entry>
    <title>VR15&amp;#58; a VR FPS game controller that generates haptic feedback with high-pressure air.</title>
    <link href="http://zhyma.github.io/VR15/"/>
    <id>http://zhyma.github.io/VR15/</id>
    <published>2021-12-30T01:34:58.000Z</published>
    <updated>2023-12-25T17:58:28.609Z</updated>
    
    <content type="html"><![CDATA[<p>This project is covered by Hackaday. You can find the article <a href="https://hackaday.com/2022/01/05/haptic-feedback-rifle-lets-you-take-aim-in-vr/" target="_blank" rel="noopener">here</a><br>The project is open-source. You can find the hardware design <a href="https://github.com/zhyma/vr15" target="_blank" rel="noopener">here</a>, and the a demo game <a href="https://github.com/zhyma/rush_b" target="_blank" rel="noopener">here</a>.<br><br /></p><ul><li>You can find the short demo here (<a href="https://www.youtube.com/watch?v=LVkTlivSAAY" target="_blank" rel="noopener">YouTube link</a>):<iframe width="560" height="315" src="https://www.youtube.com/embed/LVkTlivSAAY?si=j7K6PRgrkcEglR0A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></li><li>The full video that explain the design is also on <a href="https://www.youtube.com/watch?v=NAWumtR1TvY" target="_blank" rel="noopener">YouTube</a>.</li></ul><h2 id="Key-Features"><a href="#Key-Features" class="headerlink" title="Key Features"></a>Key Features</h2><ul><li><p>The haptic feedback is provided by controlled high-pressure air and programmable.<br><div style="display:inline-block;text-align:center;"><br><img src="/img/vr15/ezrecoil.gif" alt=""><br></div><br><br /></p></li><li><p>The displacement of the charging handle is being tracked real-time.<br><div style="display:inline-block;text-align:center;"><br><img src="/img/vr15/charging.gif" alt=""><br></div><br><br /></p></li><li><p>You can pull the charging handle to check if it is “loaded”.<br><div style="display:inline-block;text-align:center;"><br><img src="/img/vr15/check.gif" alt=""><br></div><br><br /></p></li><li><p>The whole human-machine interface of AR15 is tracked and can be reflected on the model in the game.<br><br /></p><ul><li><p>The safe/semi/auto switch.</p>  <div style="display:inline-block;text-align:center;"><br>  <img src="/img/vr15/semiauto.gif" alt=""><br>  </div></li><li><p>The magazine.</p>  <div style="display:inline-block;text-align:center;"><br>  <img src="/img/vr15/tracking.gif" alt=""><br>  </div></li><li><p>The trigger.<br>  <div style="display:inline-block;text-align:center;"><br>  <img src="/img/vr15/trigger.gif" alt=""><br>  </div><br><br /></p></li></ul></li><li><p>Simulate AR’s hold open.<br><div style="display:inline-block;text-align:center;"><br><img src="/img/vr15/holdopen.gif" alt=""><br></div><br><div style="display:inline-block;text-align:center;"><br><img src="/img/vr15/hold_open_demo.gif" alt=""><br></div><br><br /></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/vr15/vr15.jpg&quot;&gt; A highly customized high-pressure air blow back airsoft that simulates recoil power and human-machine interface of an AR15 to brings high-fidelity experience for VR FPS gaming.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="VR" scheme="http://zhyma.github.io/tags/VR/"/>
    
  </entry>
  
  <entry>
    <title>Enhanced tele-operation camera control by head motions</title>
    <link href="http://zhyma.github.io/Enhanced-tele-operation-camera-control-by-head-motions/"/>
    <id>http://zhyma.github.io/Enhanced-tele-operation-camera-control-by-head-motions/</id>
    <published>2019-09-18T03:42:07.000Z</published>
    <updated>2023-12-25T18:15:19.455Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong>: Shang Gao, Ryosuke Tsumura, Jakub Kaminski, Loris Fichera, Haichong K Zhang</p><p><strong>This work is published in SPIE Medical Imaging 2021. You can find out more information <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11598/1159815/Augmented-immersive-telemedicine-through-camera-view-manipulation-controlled-by-head/10.1117/12.2581599.short" target="_blank" rel="noopener">here</a></strong></p><div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/demo.png" alt=""><br></div><br>Traditional method that using head motion to control a remote camera maps the position and orientation directly or with scale factor. This is useful for a searching task, but not utilized for tasks such as examine an object.<br><br>We proposes a hands-free approach to control the camera view for an improved telepresence experience. The system comprises an RGBD camera mounted on a robotic arm and a motion tracking virtual reality (VR) head mount display (HMD) maps the human head and upper-body motion to the robotic arm for the immersive teleoperation task.<br><br><div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/sys_arch.jpg" alt=""><br><em>System block diagram</em><br></div><p>Based on this setup, an Augmented Head Motion Mapping (AHMM) mode is introduced. Wherein this mode, the user can decide to control the camera following the head motion directly or following the remote center of motion (RCM) to the target location so that the reachable visual field can be expanded. </p><div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/animation.gif" alt=""><br><em>RCM mapping</em><br></div><p>We compared this AHMM with Direct Head Motion Mapping. A comparison demo can be found below:</p><div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/live_demo.gif" alt=""><br><em>The corresponding motion of the camera when the user performs<br/> the same motion under different control mode.</em><br></div><p>Through the user study with seven subjects, we evaluated the proposed method compared with other conventional methods in terms of the reachable visual field, control intuitiveness, and task efficiency. The possibility of further enlarging the reachable visual field by introducing the motion scaling factor is investigated through the simulation. The result successfully demonstrated that an operator using the proposed system could examine a larger area on the given object within a similar amount of time with limited training.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/cam_ctrl/vr_me.jpg&quot;&gt; A remote camera position and orientation controlled by a operator&#39;s head and upper body motion. The operator could switch between different motion mapping strategies to enlarge the reachable visual field of the camera with limited range of motion.&lt;br /&gt;&lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11598/1159815/Augmented-immersive-telemedicine-through-camera-view-manipulation-controlled-by-head/10.1117/12.2581599.short&quot;&gt;&lt;b&gt;Related work&lt;/b&gt;&lt;/a&gt; is published on Medical Image 2021&amp;#58; Image-Guided Procedures, Robotic Interventions, and Modeling.&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Robotics" scheme="http://zhyma.github.io/tags/Robotics/"/>
    
      <category term="VR" scheme="http://zhyma.github.io/tags/VR/"/>
    
  </entry>
  
  <entry>
    <title>A linear force-sensing handbrake for sim-racing</title>
    <link href="http://zhyma.github.io/A-linear-force-sensing-handbrake-for-sim-racing/"/>
    <id>http://zhyma.github.io/A-linear-force-sensing-handbrake-for-sim-racing/</id>
    <published>2018-02-11T04:42:07.000Z</published>
    <updated>2023-12-25T17:53:21.392Z</updated>
    
    <content type="html"><![CDATA[<div style="display:inline-block;text-align:center;"><br><img src="/img/handbrake/quat.jpg" alt=""><br></div><p>You can find the demo and a guide of assembly in the video (<a href="https://www.youtube.com/watch?v=2aMNOjdhkCo" target="_blank" rel="noopener">YouTube link</a>)</p><iframe width="560" height="315" src="https://www.youtube.com/embed/2aMNOjdhkCo?si=gfpqtQpLJ06rXjBt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><p>You can find the CAD file, source code, and how you can make one on my <a href="https://github.com/zhyma/force_sensing_handbrake" target="_blank" rel="noopener">GitHub repo</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/handbrake/handbrake.jpg&quot;&gt; A handbrake for sim-racing. Using load cell for linear force-sensing.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="SimRacing" scheme="http://zhyma.github.io/tags/SimRacing/"/>
    
  </entry>
  
  <entry>
    <title>Chatting robot with behavior learning</title>
    <link href="http://zhyma.github.io/Chatting-robot-with-behavior-learning/"/>
    <id>http://zhyma.github.io/Chatting-robot-with-behavior-learning/</id>
    <published>2017-07-21T03:42:07.000Z</published>
    <updated>2021-04-08T01:44:48.184Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Katsushi Ikeuchi, David Baumert, Yutaka Suzue, Masaru Takizawa, Kazuhiro Sasabuchi, et al. </p><p><strong>My contribution:</strong></p><ul><li>System design and implementation (on top of ROS).</li></ul><p><strong>Related work was demoed at “Microsoft Research Faculty Summit 2017: The Edge of AI” (<a href="https://www.microsoft.com/en-us/research/event/microsoft-research-asia-academic-day-2017/" target="_blank" rel="noopener">Automatic Description of Human Motion and Its Reproduction by Robot Based on Labanotation</a>) and “2017 MSRA academic day”(<a href="https://www.microsoft.com/en-us/research/video/video-abstract-human-robot-collaboration/" target="_blank" rel="noopener">Human-Robot Collaboration</a>).</strong></p><p>We propose a cloud-based system to empower service robots to generate human-like motions corresponding to the on-going conversation while chatting with people. In conversation, a gesture along with spoken sentence is an important factor as it is referred to as body language. This is particularly true for humanoid service robots because the key merit of such a humanoid robot is its resemblance to human shape as well as human behavior. Currently, robots with physical forms are not expected to reply with gestures when encountering contents outside their fixed knowledge. To fill this gap, we design a working prototype by employing a SoftBank Pepper robot and Microsoft Cognitive Services, shown in the diagram below.</p><p><img src="/img/msrabot/diag1.png" alt=""></p><p>To enable the gesture synthesis function, we build a “gesture library”. First, it contains common gestures. Second, it can analyze a sentence to map it a concept within the library. Third, it searches in the library for the concept, select a suitable gesture, which is written in Labanotation, to drive robots. The system diagram is shown below.</p><p><img src="/img/msrabot/diag2.png" alt=""></p><p>To prepare the library, we collected common gestures of daily conversations by using our Learning-from-Observation system (you can find more details <a href="/Robot-learning-from-observation/" title="here">here</a>), clustered them into about 40 concepts and designed a mapping mechanism to pair given sentences to concepts. To implement a complete system, we also employed Microsoft face tracking, speech recognition, chatting engine and text to speech services. Thus, our robots are able to process conversations with humans and automatically generate meaningful physical behaviors to accompany its spoken words.</p><p>You can find more details about this project from <a href="https://www.microsoft.com/en-us/research/video/visual-sensing-visual-intelligence/" target="_blank" rel="noopener">Katsu’s talk</a> at Microsoft Research Faculty Summit 2017 (starts at 48:26). Related slides can be found <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/07/From_Visual_Sensing_to_Visual_Intelligence_Katsu_Ikeuchi.pdf" target="_blank" rel="noopener">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/msrabot/pepper.jpg&quot;&gt; A robot system that is able to automatically generate life-like and meaningful physical behaviors to accompany its spoken words when process conversations with humans.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Robotics" scheme="http://zhyma.github.io/tags/Robotics/"/>
    
  </entry>
  
  <entry>
    <title>Robot learning from observation</title>
    <link href="http://zhyma.github.io/Robot-learning-from-observation/"/>
    <id>http://zhyma.github.io/Robot-learning-from-observation/</id>
    <published>2016-05-26T14:25:52.000Z</published>
    <updated>2023-12-25T18:09:58.976Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Teamwork with</strong> Katsushi Ikeuchi, Zengquang Yan, Yoshihiro Sato, Minako Nakamura, Shunsuke Kudho, et al.</p><p><strong>My contribution:</strong></p><ul><li>Proposed a scale-space filtering-based method to extract keyframes from human gestures to generate a representation for reproducing human motion. When using a Labanotator’s notation as the baseline, this method performs better than the previous energy-function-based method in terms of accuracy.</li><li>Algorithm implementation.</li><li>Robot control programming under ROS.</li><li>Robot hardware design and prototyping.</li></ul><p><strong>Related work is published in International Journal of Computer Vision, 2018. You can find out more information <a href="http://link.springer.com/article/10.1007/s11263-018-1123-1" target="_blank" rel="noopener">here</a>.</strong><br><strong>Related work was demoed to Bill Gates, Paul Allen and Satya Nadella at TechFest 2016 of Microsoft.</strong></p><div style="display:inline-block;text-align:center"><img src="/img/msrabot/robot@facebook2.png" alt=""><br><em>Related post on Harry Shum’s facebook</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/msrabot/robot3.jpg" alt=""><br><em>Hand-made robot prototype</em><br></div><p>We present a robot interaction system by introducing <a href="https://en.wikipedia.org/wiki/Labanotation" target="_blank" rel="noopener">Labanotation</a> into the working process. In real world scenario, there are several ways of programming a robot. But due to the kinematic and dynamic difference between human and robots, or even different robots themselves, simple mimicking method to repeat exactly the same joint angles will not work. So we bring Labanotation, a notation being used to record dance, as an intermediate symbolic representation for imitation of human’s upper body motion. In our system, a person performs series of actions in front of a Kinect first. Human skeleton information is captured and processed into so-called “energy function”. Peaks in the energy function are detected and treated as time point of key poses. A sequence of Labanotation is generated from the skeleton information (a.k.a. key pose) of the corresponding time points. After that, the Labanotation is sent to different robots. Robots receive the message then interpret Labanotation according to its’ own kinematics structure to mimic human action.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/msrabot/flowchart.png" alt=""><br><em>Procedures for using Labanotation as intermediate language of robot control.</em><br></div><p>For tasks that do not request precise manipulation, the biggest benefits of doing this is time-saving for motion editing to different robots. Engineers only need to program the robot once to set up rules of mapping Labanotation symbols. Then robots are able to interpret any Labanotation sequences.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/msrabot/robot.jpg&quot;&gt;Robots learn human motion by observing human behavior.&lt;br /&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/s11263-018-1123-1&quot;&gt;&lt;b&gt;Related work&lt;/b&gt;&lt;/a&gt; is published on International Journal of Computer Vision.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
      <category term="Robotics" scheme="http://zhyma.github.io/tags/Robotics/"/>
    
  </entry>
  
  <entry>
    <title>Touch typing on touch cover</title>
    <link href="http://zhyma.github.io/Touch-typing-on-touch-cover/"/>
    <id>http://zhyma.github.io/Touch-typing-on-touch-cover/</id>
    <published>2015-07-07T15:03:41.000Z</published>
    <updated>2023-12-25T18:07:10.719Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Darren Edge, Leah Findlater, Hong Z. Tan</p><p><strong>My contribution:</strong></p><ul><li>Invent a mechanical structure to use isolated piezoelectric ceramic as actuators to provide haptic feedback for keys on a 3mm-thick-keyboard independently.</li><li>Mechanical structure, control circuit, and firmware design and prototyping.</li><li>Design and conducting a user study based on the keyboard to evaluate users’ experience of typing with this design.</li></ul><p><strong>Related work on this keyboard is published in Proceedings of IEEE World Haptics Conference (WHC 2015), you can find out more information <a href="https://www.microsoft.com/en-us/research/publication/haptic-keyclick-feedback-improves-typing-speed-and-reduces-typing-errors-on-a-flat-keyboard/" target="_blank" rel="noopener">here</a></strong></p><p>I received a patent for this design: “Hong Z. Tan, Zhaoyuan Ma &amp; Chen Zhao. LOCALIZED KEY-CLICK FEEDBACK. WO/2014/186428, published 11/20/2014, filed 05/14/2014.”</p><div style="display:inline-block;text-align:center;"><br><object width="560" height="315" align="aligncenter" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true"><param name="allowscriptaccess" value="always"><param name="src" value="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=100"><param name="allowfullscreen" value="true"><embed width="560" height="315" type="application/x-shockwave-flash" src="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=100" allowfullscreen="true" allowscriptaccess="always"></object><br><em>Demostrated on Microsoft TechFest 2014 (<a href="https://www.youtube.com/watch?v=ee1wuZxiLsc&amp;t=100s" target="_blank" rel="noopener">YouTube video</a>)</em><br></div><p>People prefer Microsoft Type Cover rather than Touch Cover regardless of the thickness difference, due to Type Cover provide a better feeling of haptic feedback. Although this Cover shares the exact same appearance and dimension with other Touch Covers, everything inside has been pulled out and replaced by a totally different structure. Piezoelectric actuators are used to simulate physical keyboards, which means to create a sense of click under the certain finger that is pressing down. The study shows that people type faster and make fewer errors when they type on this kind of key-click as compared to just sound feedback.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/keyboard1.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/IMG_4783.jpg" alt=""><br><em>Our implementation: black. Original Touch Cover: blue</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/IMG_4794.jpg" alt=""><br><em>Sideview</em><br></div><div style="display:inline-block;text-align:center;"><br><img src="/img/keyboard/insight.jpg" alt=""><br><em>The inside view of the keyboard structure (modified version for user study)</em><br></div><p><strong>Keyboard Feature:</strong></p><ul><li>Same form factor as Microsoft Touch Cover.</li><li>Isolated haptic feedback under each key.</li><li>Using piezo ceramic for both sensing and actuating.</li><li>Haptic feedback delay: 40.2ms.</li><li>Standard USB keyboard interface (through MicroUSB connection).</li><li>With configuration tool to turn on/off haptic feedback or change the feedback waveform.</li></ul><p><strong>User Study Take Away:</strong></p><ul><li>Haptic feedback improves typing speed and reduces errors</li><li>Audio feedback helps, but only if there is no haptics</li><li>Haptic feedback alone is superior to audio feedback alone</li><li>Users prefer localized haptics, but global helps home positioning</li></ul><p>Special thanks to Magnetro Chen for his help!</p><p>I proceeded a user study with 24 participants, more than 60 hours in total.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/keyboard/keyboard.jpg&quot;&gt;This is the world first flat keyboard (without moving keys) that can deliver haptic keyclick feedback felt locally on each key. This keyboard has the same form factor of a Microsoft Touch Cover. We conducted a study to evaluate how haptic keyclick feedback might improve  typing  performance.&lt;br /&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/7177717&quot;&gt;&lt;b&gt;Related work&lt;/b&gt;&lt;/a&gt; is published on Proceedings of IEEE World Haptics Conference (WHC 2015).
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
  <entry>
    <title>eClover</title>
    <link href="http://zhyma.github.io/eClover/"/>
    <id>http://zhyma.github.io/eClover/</id>
    <published>2015-07-07T14:37:45.000Z</published>
    <updated>2020-07-22T18:41:19.733Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Yukang Yan, Darren Edge, Hong Z. Tan, Yuanchun Shi and Ed Colgate</p><p><strong>My contribution:</strong></p><ul><li>Circuit (tactors controlling) design, assembling and debugging.</li><li>Software modification.</li></ul><p><img src="/img/eclover/eclover-sys1.jpg" alt=""></p><p>We present a wearable prototype combining an electrostatic interface with four tactors worn around the wrist. This combination allows users to feel the approximate time in situations where glancing might not be appropriate, such as when in a meeting or giving a presentation. Users can also elect to receive notifications for scheduled events or at regular time intervals throughout the day.</p><p><strong>Driving circuit</strong><br>The driver for the electrostatic display and tactors.</p><p><strong>Electrostatic display</strong><br>The electrostatic display on the upper surface allows users to “pull” information at any time. The system provides relative information such as time elapsed by adjusting the extent of high and low friction areas along the length of the display.</p><p><strong>Tactors around the wrist</strong><br>Four tactors are arranged around the wrist, each controlled individually. We use the relative position of the signal generated by the tactors and electrostatic display to notify the user of certain timing milestones. We also use the cutaneous saltatory effect to generate vibration patterns that could be used to announce the time, e.g., every half hour.</p><p>This project is presented as a Demo in the <a href="http://haptics2015.org/program/index.html#Demonstrations" target="_blank" rel="noopener">IEEE World Haptics Conference 2015</a>.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/eclover/eclover-sys2.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><p><strong>Early prototypes:</strong></p><div style="display:inline-block;text-align:center;"><br><img src="/img/eclover/827A1663.jpg" alt=""><br><em>Up: tactors and their control unit.<br>Down: enclosed with leather (hand made)</em><br></div>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/eclover/eclover.jpg&quot;&gt;A wearable prototype combining an electrostatic interface with four tactors worn around the wrist. This combination allows users to feel the approximate time in situations where glancing might not be appropriate, such as when in a meeting or giving a presentation. Users can also elect to receive notifications for scheduled events or at regular time intervals throughout the day.
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
  <entry>
    <title>Haptics feedback on a touch screen</title>
    <link href="http://zhyma.github.io/Haptics-feedback-on-a-touch-screen/"/>
    <id>http://zhyma.github.io/Haptics-feedback-on-a-touch-screen/</id>
    <published>2014-05-04T03:33:47.000Z</published>
    <updated>2020-07-22T18:41:24.853Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Co-author:</strong> Chen Zhao, Xiaowei Dai, Kwangtaek Kim, Jiawei Gu, Peter Choi, Hong Z. Tan, Ed Colgate</p><p><strong>My contribution:</strong></p><ul><li>Physical simulation</li><li>Circuit debugging</li><li>Piezoelectric actuators impedance matching</li><li>Firmware/APP programming</li></ul><div style="display:inline-block;text-align:center;"><br><object width="560" height="315" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true"><param name="allowscriptaccess" value="always"><param name="src" value="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=137"><param name="allowfullscreen" value="true"><embed width="560" height="315" type="application/x-shockwave-flash" src="//www.youtube.com/v/ee1wuZxiLsc?version=3&amp;hl=en_US&amp;start=137" allowfullscreen="true" allowscriptaccess="always"></object><br><em>Demostrated on Microsoft TechFest 2014 (Youtube video)</em><br></div><p>SlickFeel-Lumia is an advanced version of SlickFeel. A Lumia 920, a piece of glass with piezo actuators, circuit boards and a battery are boxed together to implement a handheld haptic feedback device. This device could generate different friction based on TPaD (tactile pattern display) technic, which allows users to create a different feeling on bare fingers for scenarios. For examples, switching between selections, scrolling on bars, moving out of range. The localized feeling it provides distinguished this device from phones using vibrators as haptic feedback.</p><div style="display:inline-block;text-align:center;"><br><img src="/img/lumia/projLumia.jpg" alt="System block diagram"><br><em>System block diagram</em><br></div><p><img src="/img/lumia/DSC07467.jpg" alt=""></p><p><img src="/img/lumia/DSC07552.jpg" alt=""></p><p>Special thanks to Prof. Ed Colgate and the <a href="http://tpadtablet.org/" target="_blank" rel="noopener">TPaD</a> Tablet team for sharing their knowledge on TPaD with us!<br>This project is accepted as a Demo in the <a href="http://www.haptics2013.org/sub03_07.php" target="_blank" rel="noopener">IEEE World Haptics Conference 2013</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/lumia/lumia.jpg&quot;&gt;A handheld haptic feedback device which could generate different friction based on TPaD (tactile pattern display) technic, allow users to create a different feeling on bare fingers for scenarios. For examples, switching between selections, scrolling on bars, moving out of range. The localized feeling it provides distinguished this device from phones using vibrators as haptic feedback.
    
    </summary>
    
      <category term="Project" scheme="http://zhyma.github.io/categories/Project/"/>
    
    
      <category term="Haptics" scheme="http://zhyma.github.io/tags/Haptics/"/>
    
      <category term="Embedded system" scheme="http://zhyma.github.io/tags/Embedded-system/"/>
    
  </entry>
  
</feed>
