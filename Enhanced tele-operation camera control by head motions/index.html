<!A-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Zhaoyuan Ma"><meta name="description" content="&lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/cam_ctrl/vr_me.jpg&quot;&gt; A remote camera position and orientation controlled by a operator's head and upper body motion. The operator could switch between different motion mapping strategies to enlarge the reachable visual field of the camera with limited range of motion.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;"><meta name="keywords" content="Robotics,VR"><title>Enhanced tele-operation camera control by head motions Â· Zhaoyuan Ma</title><!--link(rel="icon", href=url_for(theme.favicon))--><link rel="canonical" href="http://zhyma.github.io/Enhanced%20tele-operation%20camera%20control%20by%20head%20motions/"><link rel="alternate" href="/atom.xml" title="Zhaoyuan Ma"><link rel="stylesheet" href="/fonts/iconfont/iconfont.css"><link rel="stylesheet" href="/css/style.css"><script type="text/javascript">var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?Your baidu Analytics ID";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'Your Google Analytics ID', 'auto');
ga('send', 'pageview');</script></head><body><div id="main"><header><a href="/." class="logo">Zhaoyuan Ma</a><ul class="nav"><li class="nav-link"><a href="/" target="_self">Home</a></li><li class="nav-link"><a href="/about/" target="_self">About</a></li></ul></header><section id="container"><article class="post"><h1 class="post-title">Enhanced tele-operation camera control by head motions</h1><span class="post-time">Sep 17, 2019</span><div class="post-content"><p><strong>Teamwork with</strong>: Shang Gao, Ryosuke Tsumura, Jakub Kaminski, Loris Fichera, Haichong K Zhang</p>
<p><strong>This work is published in SPIE Medical Imaging 2021. You can find out more information <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11598/1159815/Augmented-immersive-telemedicine-through-camera-view-manipulation-controlled-by-head/10.1117/12.2581599.short" target="_blank" rel="noopener">here</a></strong></p>
<div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/demo.png" alt=""><br></div><br>Traditional method that using head motion to control a remote camera maps the position and orientation directly or with scale factor. This is useful for a searching task, but not utilized for tasks such as examine an object.<br><br>We proposes a hands-free approach to control the camera view for an improved telepresence experience. The system comprises an RGBD camera mounted on a robotic arm and a motion tracking virtual reality (VR) head mount display (HMD) maps the human head and upper-body motion to the robotic arm for the immersive teleoperation task.<br><br><div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/sys_arch.jpg" alt=""><br><em>System block diagram</em><br></div>

<p>Based on this setup, an Augmented Head Motion Mapping (AHMM) mode is introduced. Wherein this mode, the user can decide to control the camera following the head motion directly or following the remote center of motion (RCM) to the target location so that the reachable visual field can be expanded. </p>
<div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/animation.gif" alt=""><br><em>RCM mapping</em><br></div>

<p>We compared this AHMM with Direct Head Motion Mapping. A comparison demo can be found below:</p>
<div style="display:inline-block;text-align:center;"><br><img src="/img/cam_ctrl/live_demo.gif" alt=""><br><em>The corresponding motion of the camera when the user performs<br/> the same motion under different control mode.</em><br></div>

<p>Through the user study with seven subjects, we evaluated the proposed method compared with other conventional methods in terms of the reachable visual field, control intuitiveness, and task efficiency. The possibility of further enlarging the reachable visual field by introducing the motion scaling factor is investigated through the simulation. The result successfully demonstrated that an operator using the proposed system could examine a larger area on the given object within a similar amount of time with limited training.</p>
</div></article><div class="tags"><a href="/tags/Robotics/">Robotics</a><a href="/tags/VR/">VR</a></div><div class="paginator"><a href="/A%20linear%20force-sensing%20handbrake%20for%20sim-racing/" class="next"><span>Next</span><i class="iconfont icon-right"></i></a></div></section><footer><div class="copyright"><p class="power">Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> and Theme by <a href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener"> Even</a></p><p class="since">&copy;2012-2023<span class="at"> @ </span><span class="author">Zhaoyuan Ma</span></p></div><label id="back2top"><i class="iconfont icon-up"></i></label></footer></div><script src="/js/zepto.min.js"></script><script src="/js/theme.js"></script></body></html>