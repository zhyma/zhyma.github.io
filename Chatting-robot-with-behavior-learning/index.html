<! lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Zhaoyuan Ma"><meta name="description" content="&lt;img align=&quot;left&quot; style=&quot;padding-right:1em&quot; src=&quot;/img/msrabot/pepper.jpg&quot;&gt; A robot system that is able to automatically generate life-like and meaningful physical behaviors to accompany its spoken words when process conversations with humans.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;"><meta name="keywords" content="Robotics"><title>Chatting robot with behavior learning · Zhaoyuan Ma</title><!--link(rel="icon", href=url_for(theme.favicon))--><link rel="canonical" href="http://zhyma.github.io/Chatting-robot-with-behavior-learning/"><link rel="alternate" href="/atom.xml" title="Zhaoyuan Ma"><link rel="stylesheet" href="/fonts/iconfont/iconfont.css"><link rel="stylesheet" href="/css/style.css"><script type="text/javascript">var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?Your baidu Analytics ID";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'Your Google Analytics ID', 'auto');
ga('send', 'pageview');</script></head><body><div id="main"><header><a href="/." class="logo">Zhaoyuan Ma</a><ul class="nav"><li class="nav-link"><a href="/" target="_self">Home</a></li><li class="nav-link"><a href="https://github.com/zhyma" target="_self">GitHub</a></li><li class="nav-link"><a href="https://www.thingiverse.com/zhyma/designs" target="_self">Thingiverse</a></li><li class="nav-link"><a href="https://www.youtube.com/channel/UCQC6SYbDgg6KY1AtMet4SdQ" target="_self">Youtube</a></li><li class="nav-link"><a href="/about/" target="_self">About</a></li></ul></header><section id="container"><article class="post"><h1 class="post-title">Chatting robot with behavior learning</h1><span class="post-time">Jul 20, 2017</span><div class="post-content"><p><strong>Co-author:</strong> Katsushi Ikeuchi, David Baumert, Yutaka Suzue, Masaru Takizawa, Kazuhiro Sasabuchi, et al. </p>
<p><strong>My contribution:</strong></p>
<ul>
<li>System design and implementation (on top of ROS).</li>
</ul>
<p><strong>Related work was demoed at “Microsoft Research Faculty Summit 2017: The Edge of AI” (<a href="https://www.microsoft.com/en-us/research/event/microsoft-research-asia-academic-day-2017/" target="_blank" rel="noopener">Automatic Description of Human Motion and Its Reproduction by Robot Based on Labanotation</a>) and “2017 MSRA academic day”(<a href="https://www.microsoft.com/en-us/research/video/video-abstract-human-robot-collaboration/" target="_blank" rel="noopener">Human-Robot Collaboration</a>).</strong></p>
<p>We propose a cloud-based system to empower service robots to generate human-like motions corresponding to the on-going conversation while chatting with people. In conversation, a gesture along with spoken sentence is an important factor as it is referred to as body language. This is particularly true for humanoid service robots because the key merit of such a humanoid robot is its resemblance to human shape as well as human behavior. Currently, robots with physical forms are not expected to reply with gestures when encountering contents outside their fixed knowledge. To fill this gap, we design a working prototype by employing a SoftBank Pepper robot and Microsoft Cognitive Services, shown in the diagram below.</p>
<p><img src="/img/msrabot/diag1.png" alt=""></p>
<p>To enable the gesture synthesis function, we build a “gesture library”. First, it contains common gestures. Second, it can analyze a sentence to map it a concept within the library. Third, it searches in the library for the concept, select a suitable gesture, which is written in Labanotation, to drive robots. The system diagram is shown below.</p>
<p><img src="/img/msrabot/diag2.png" alt=""></p>
<p>To prepare the library, we collected common gestures of daily conversations by using our Learning-from-Observation system (you can find more details <a href="/Robot-learning-from-observation/" title="here">here</a>), clustered them into about 40 concepts and designed a mapping mechanism to pair given sentences to concepts. To implement a complete system, we also employed Microsoft face tracking, speech recognition, chatting engine and text to speech services. Thus, our robots are able to process conversations with humans and automatically generate meaningful physical behaviors to accompany its spoken words.</p>
<p>You can find more details about this project from <a href="https://www.microsoft.com/en-us/research/video/visual-sensing-visual-intelligence/" target="_blank" rel="noopener">Katsu’s talk</a> at Microsoft Research Faculty Summit 2017 (starts at 48:26). Related slides can be found <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/07/From_Visual_Sensing_to_Visual_Intelligence_Katsu_Ikeuchi.pdf" target="_blank" rel="noopener">here</a>.</p>
</div></article><div class="tags"><a href="/tags/Robotics/">Robotics</a></div><div class="paginator"><a href="/A linear force-sensing handbrake for sim-racing/" class="prev"><i class="iconfont icon-left"></i><span> Prev</span></a><a href="/Robot-learning-from-observation/" class="next"><span>Next</span><i class="iconfont icon-right"></i></a></div></section><footer><div class="copyright"><p class="power">Powered by <a href="https://hexo.io/">Hexo</a> and Theme by <a href="https://github.com/ahonn/hexo-theme-even"> Even</a></p><p class="since">&copy;2012-2022<span class="at"> @ </span><span class="author">Zhaoyuan Ma</span></p></div><label id="back2top"><i class="iconfont icon-up"></i></label></footer></div><script src="/js/zepto.min.js"></script><script src="/js/theme.js"></script></body></html>